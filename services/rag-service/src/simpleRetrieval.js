

function tokenize(text) {
  return text
    .toLowerCase()
    .replace(/[^a-z0-9\s]/g, ' ')
    .split(/\s+/)
    .filter(Boolean);
}

function scoreDocuments(question, documents) {
  const questionTokens = new Set(tokenize(question));

  return documents
    .map((doc) => {
      const docTokens = tokenize(doc.content);
      let score = 0;
      for (const t of docTokens) {
        if (questionTokens.has(t)) {
          score += 1;
        }
      }
      return { document: doc, score };
    })
    .sort((a, b) => b.score - a.score);
}

function generateAnswer(question, topDocs) {
  const intro =
    'This is a simple RAG-style answer generated by DocBrain based on your indexed documents.\n\n';
  const qLine = `Question: ${question}\n\n`;
  if (!topDocs || topDocs.length === 0) {
    return intro + qLine + 'I could not find any relevant content in the indexed documents.';
  }

  let contextSummary = 'Relevant documents used:\n';
  topDocs.forEach((doc, idx) => {
    contextSummary += `  ${idx + 1}. ${doc.title} (id: ${doc.document_id})\n`;
  });

  contextSummary += '\nExtracted snippets:\n';

  topDocs.forEach((doc, idx) => {
    const contentPreview =
      doc.content.length > 300 ? doc.content.slice(0, 300) + '...' : doc.content;
    contextSummary += `\n[${idx + 1}] ${doc.title}\n${contentPreview}\n`;
  });

  const closing =
    '\n\n(This demo does not use a real LLM; in a full implementation, this context would be sent to an LLM to generate a natural-language answer.)';

  return intro + qLine + contextSummary + closing;
}

module.exports = { scoreDocuments, generateAnswer };
